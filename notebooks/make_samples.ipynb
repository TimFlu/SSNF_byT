{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hep_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2, expon, weibull_min\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from hep_ml import reweight\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc798a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevs = 2000000\n",
    "dof = 3\n",
    "############################################ sample 1 #########################################################\n",
    "pdf1 = chi2(dof)\n",
    "# sample the chi2 distribution\n",
    "sample1 = pdf1.rvs(nevs)\n",
    "# set the lowest entry to 0 and the highest to 1, then scale all the other values in between\n",
    "sample1 = MinMaxScaler((0, 1)).fit_transform(sample1.reshape(-1, 1)).reshape(-1)\n",
    "# create the fake targed (aka the data) with some introduced noise\n",
    "print(f\"shape of sample1: {sample1.shape}\")\n",
    "sample1_target = sample1 + 0.1 + 0.01 * np.random.normal(size=sample1.shape)\n",
    "# split target1 sample into training and test samples\n",
    "sample1_train, sample1_test = train_test_split(sample1, test_size=0.3)\n",
    "# split the target data into training and test samples\n",
    "sample1_target_train, sample1_target_test = train_test_split(sample1_target, test_size=0.3)\n",
    "\n",
    "# plot the training sample of the MC (sample1) and data (sample1_target)\n",
    "plt.hist(sample1_train, bins=100, label=\"MC\")\n",
    "plt.hist(sample1_target_train, bins=100, alpha=0.5, label=\"data\")\n",
    "plt.xlabel('a')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "############################################ sample 2 ###########################################################\n",
    "pdf2 = weibull_min(10, 1)\n",
    "sample2 = pdf2.rvs(nevs)\n",
    "# !! Introducing some correlation by using sample1 to define sample2\n",
    "sample2 = sample2 - 5*sample1\n",
    "sample2 = MinMaxScaler((0, 1)).fit_transform(sample2.reshape(-1, 1)).reshape(-1)\n",
    "sample2_target = sample2 - 0.05 + 0.01 * np.random.normal(size=sample2.shape)\n",
    "sample2_train, sample2_test = train_test_split(sample2, test_size=0.3)\n",
    "sample2_target_train, sample2_target_test = train_test_split(sample2_target, test_size=0.3)\n",
    "plt.hist(sample2_train, bins=100, label=\"MC\")\n",
    "plt.hist(sample2_target_train, bins=100, alpha=0.5, label=\"data\")\n",
    "plt.legend()\n",
    "plt.xlabel('b')\n",
    "plt.show()\n",
    "\n",
    "############################################ sample 3 ###########################################################\n",
    "# !! Introducing correlation by using sample1 and sample2 to define sample3\n",
    "sample3 = np.log1p(sample1) + np.log1p(sample2)\n",
    "sample3 = MinMaxScaler((0, 1)).fit_transform(sample3.reshape(-1, 1)).reshape(-1)\n",
    "sample3_target = np.log1p(sample1_target) + np.log1p(sample2_target)\n",
    "sample3_target = QuantileTransformer(output_distribution='normal').fit_transform(sample3_target.reshape(-1, 1)).reshape(-1)\n",
    "sample3_target = MinMaxScaler((0, 1)).fit_transform(sample3_target.reshape(-1, 1)).reshape(-1)\n",
    "sample3_train, sample3_test = train_test_split(sample3, test_size=0.3)\n",
    "sample3_target_train, sample3_target_test = train_test_split(sample3_target, test_size=0.3)\n",
    "plt.hist(sample3_train, bins=100, label=\"MC\")\n",
    "plt.hist(sample3_target_train, bins=100, alpha=0.5, label=\"data\")\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "############################################ sample 4 ###########################################################\n",
    "# !! Introducing correlation by using sample1 and sample2 to define sample4\n",
    "sample4 = np.log1p(sample1) + np.log1p(sample2) - np.log1p(sample1 * sample2) + 0.05\n",
    "sample4 = MinMaxScaler((0, 1)).fit_transform(sample4.reshape(-1, 1)).reshape(-1)\n",
    "sample4_target = np.log1p(sample1_target) + np.log1p(sample2_target) - np.log1p(sample1_target * sample2_target) + 0.05\n",
    "sample4_target = QuantileTransformer(output_distribution='normal').fit_transform(sample4_target.reshape(-1, 1)).reshape(-1)\n",
    "sample4_target = MinMaxScaler((0, 1)).fit_transform(sample4_target.reshape(-1, 1)).reshape(-1)\n",
    "sample4_train, sample4_test = train_test_split(sample4, test_size=0.3)\n",
    "sample4_target_train, sample4_target_test = train_test_split(sample4_target, test_size=0.3)\n",
    "plt.hist(sample4_train, bins=100,label=\"MC\")\n",
    "plt.hist(sample4_target_train, bins=100, alpha=0.5, label=\"data\")\n",
    "plt.xlabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "############################################ store samples in df ##################################################\n",
    "df = pd.DataFrame({'a': sample1_train, 'b': sample2_train, 'x': sample3_train, 'y': sample4_train})\n",
    "df_target = pd.DataFrame({'a': sample1_target_train, 'b': sample2_target_train, 'x': sample3_target_train, 'y': sample4_target_train})\n",
    "\n",
    "df_test = pd.DataFrame({'a': sample1_test, 'b': sample2_test, 'x': sample3_test, 'y': sample4_test})\n",
    "df_target_test = pd.DataFrame({'a': sample1_target_test, 'b': sample2_target_test, 'x': sample3_target_test, 'y': sample4_target_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0457b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In analogy to our main problem: context vars are 4 Kinematic variables\n",
    "context_vars = ['a', 'b']\n",
    "# In analogy to our main problem: target vars are 6 Shower Shapes and 3 Isolation variables\n",
    "target_vars = ['x', 'y']\n",
    "\n",
    "reweighter = reweight.GBReweighter(\n",
    "    n_estimators=250,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1000,\n",
    "    gb_args={\"subsample\": 0.4},\n",
    ")\n",
    "\n",
    "bins_reweighter = reweight.BinsReweighter(n_bins=300, n_neighs=1.)\n",
    "\n",
    "bins_reweighter.fit(\n",
    "    df[context_vars].values,\n",
    "    df_target[context_vars].values,\n",
    ")\n",
    "\n",
    "train_weights = bins_reweighter.predict_weights(\n",
    "    df[context_vars].values,\n",
    ")\n",
    "\n",
    "test_weights = bins_reweighter.predict_weights(\n",
    "    df_test[context_vars].values,\n",
    ")\n",
    "\n",
    "# add weights to mc dataframe and ones to data dataframe\n",
    "df[\"weight\"] = train_weights\n",
    "df_target[\"weight\"] = np.ones(len(df_target))\n",
    "df_test[\"weight\"] = test_weights\n",
    "df_target_test[\"weight\"] = np.ones(len(df_target_test))\n",
    "\n",
    "# plot\n",
    "for var in context_vars + target_vars:\n",
    "    plt.hist(df_test[var], bins=100, density=True, weights=df_test['weight'])\n",
    "    plt.hist(df_target_test[var], bins=100, alpha=0.5, density=True)\n",
    "    plt.xlabel(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./samples\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "pickle.dump(bins_reweighter, open('./samples/reweighter.pkl', \"wb\"))\n",
    "\n",
    "# save as parquet the four samples  \n",
    "df.to_parquet(os.path.join(output_dir, \"train.parquet\"))    \n",
    "df_target.to_parquet(os.path.join(output_dir, \"train_target.parquet\"))\n",
    "df_test.to_parquet(os.path.join(output_dir, \"test.parquet\"))\n",
    "df_target_test.to_parquet(os.path.join(output_dir, \"test_target.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5edba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMS_Thesis",
   "language": "python",
   "name": "cms_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
