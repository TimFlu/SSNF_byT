{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b32c24ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries for manipulation of data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# pytorch related\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# nflows related - nflows is a comprehensive collection of normalizing flows using PyTorch.\n",
    "import nflows\n",
    "from nflows import flows, transforms\n",
    "from nflows.distributions.base import Distribution\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.utils import torchutils\n",
    "\n",
    "# set random seed for numpy\n",
    "np.random.seed(100)\n",
    "# Checks if GPU is available or not\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25aa5932",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/CMS_Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:674\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/miniconda3/envs/CMS_Thesis/lib/python3.11/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb26a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bbf60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe,\n",
    "        context_variables,\n",
    "        target_variables,\n",
    "        device=None,\n",
    "        rows=None,\n",
    "    ):\n",
    "        self.context_variables = context_variables\n",
    "        self.target_variables = target_variables\n",
    "        self.all_variables = context_variables + target_variables\n",
    "        data = dataframe\n",
    "        if rows is not None:\n",
    "            data = data.iloc[:rows]\n",
    "        self.target = data[target_variables].values\n",
    "        self.context = data[context_variables].values\n",
    "        self.weights = data[['weight']].values\n",
    "        if device is not None:\n",
    "            self.target = torch.tensor(self.target, dtype=torch.float32).to(device)\n",
    "            self.context = torch.tensor(self.context, dtype=torch.float32).to(device)\n",
    "            self.weights = torch.tensor(self.weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.context) == len(self.target)\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.context[idx], self.target[idx], self.weights[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3bf34cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_target \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_target.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/CMS_Thesis/lib/python3.11/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    430\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    DataFrame\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    496\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/CMS_Thesis/lib/python3.11/site-packages/pandas/io/parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     58\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "input_dir = \"./samples\"\n",
    "df = pd.read_parquet(os.path.join(input_dir, \"train.parquet\"))\n",
    "df_target = pd.read_parquet(os.path.join(input_dir, \"train_target.parquet\"))\n",
    "df_test = pd.read_parquet(os.path.join(input_dir, \"test.parquet\"))\n",
    "df_target_test = pd.read_parquet(os.path.join(input_dir, \"test_target.parquet\"))\n",
    "\n",
    "context_vars = ['a', 'b']\n",
    "input_vars = ['x', 'y']\n",
    "rows = 100000\n",
    "rows_test = 100000\n",
    "batch_size = 1000\n",
    "print(len(df), len(df_test))\n",
    "mc_dataset_train = MyDataset(df, context_vars, input_vars, device=device, rows=rows)\n",
    "mc_loader_train = DataLoader(mc_dataset_train, batch_size=batch_size)\n",
    "data_dataset_train = MyDataset(df_target, context_vars, input_vars, device=device, rows=rows)\n",
    "data_loader_train = DataLoader(data_dataset_train, batch_size=batch_size)\n",
    "mc_dataset_test = MyDataset(df_test, context_vars, input_vars, device=device, rows=rows_test)\n",
    "mc_loader_test = DataLoader(mc_dataset_test, batch_size=batch_size)\n",
    "data_dataset_test = MyDataset(df_target_test, context_vars, input_vars, device=device, rows=rows_test)\n",
    "data_loader_test = DataLoader(data_dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dataset_train.context.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388eea4",
   "metadata": {},
   "source": [
    "# Train bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninput = len(input_vars)\n",
    "ncontext = len(context_vars)\n",
    "\n",
    "epochs = 20\n",
    "plot_every = 10\n",
    "\n",
    "class DiagonalGaussian(Distribution):\n",
    "    \"\"\"A diagonal multivariate Normal with trainable parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, shape, mean, std):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "            shape: list, tuple or torch.Size, the shape of the input variables.\n",
    "            context_encoder: callable or None, encodes the context to the distribution parameters.\n",
    "                If None, defaults to the identity function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._shape = torch.Size(shape)\n",
    "        self.mean_ = nn.Parameter(mean * torch.ones(shape).reshape(1, -1))\n",
    "        self.log_std_ = nn.Parameter(np.log(std) * torch.ones(shape).reshape(1, -1))\n",
    "        self.register_buffer(\"_log_z\",\n",
    "                             torch.tensor(0.5 * np.prod(shape) * np.log(2 * np.pi),\n",
    "                                          dtype=torch.float32),\n",
    "                             persistent=False)\n",
    "\n",
    "    def _log_prob(self, inputs, context):\n",
    "        if inputs.shape[1:] != self._shape:\n",
    "            raise ValueError(\n",
    "                \"Expected input of shape {}, got {}\".format(\n",
    "                    self._shape, inputs.shape[1:]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Compute parameters.\n",
    "        means = self.mean_\n",
    "        log_stds = self.log_std_\n",
    "\n",
    "        # Compute log prob.\n",
    "        norm_inputs = (inputs - means) * torch.exp(-log_stds)\n",
    "        log_prob = -0.5 * torchutils.sum_except_batch(\n",
    "            norm_inputs ** 2, num_batch_dims=1\n",
    "        )\n",
    "        log_prob -= torchutils.sum_except_batch(log_stds, num_batch_dims=1)\n",
    "        log_prob -= self._log_z\n",
    "        return log_prob\n",
    "\n",
    "    def _sample(self, num_samples, context):\n",
    "        means = self.mean_\n",
    "        log_stds = self.log_std_\n",
    "        stds = torch.exp(log_stds)\n",
    "        means = torchutils.repeat_rows(means, num_samples)\n",
    "        stds = torchutils.repeat_rows(stds, num_samples)\n",
    "\n",
    "        # Generate samples.\n",
    "        context_size = context.shape[0]\n",
    "        noise = torch.randn(context_size * num_samples, *\n",
    "                            self._shape, device=means.device)\n",
    "        samples = means + stds * noise\n",
    "        return torchutils.split_leading_dim(samples, [context_size, num_samples])\n",
    "\n",
    "    def _mean(self, context):\n",
    "        return self.mean\n",
    "\n",
    "def spline_inn(\n",
    "    inp_dim,\n",
    "    nodes=128,\n",
    "    num_blocks=2,\n",
    "    num_stack=3,\n",
    "    tail_bound=3.5,\n",
    "    tails=\"linear\",\n",
    "    activation=F.relu,\n",
    "    lu=0,\n",
    "    num_bins=10,\n",
    "    context_features=None,\n",
    "    dropout_probability=0.0,\n",
    "    flow_for_flow=False,\n",
    "):\n",
    "    transform_list = []\n",
    "    for i in range(num_stack):\n",
    "        transform_list += [\n",
    "            transforms.MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "                inp_dim,\n",
    "                nodes,\n",
    "                num_blocks=num_blocks,\n",
    "                tail_bound=tail_bound,\n",
    "                num_bins=num_bins,\n",
    "                tails=tails,\n",
    "                activation=activation,\n",
    "                dropout_probability=dropout_probability,\n",
    "                context_features=context_features,\n",
    "            )\n",
    "        ]\n",
    "        if lu:\n",
    "            transform_list += [transforms.LULinear(inp_dim)]\n",
    "        else:\n",
    "            transform_list += [transforms.ReversePermutation(inp_dim)]\n",
    "\n",
    "    if not (flow_for_flow and (num_stack % 2 == 0)):\n",
    "        # If the above conditions are satisfied then you want to permute back to the original ordering such that the\n",
    "        # output features line up with their original ordering.\n",
    "        transform_list = transform_list[:-1]\n",
    "\n",
    "    return transforms.CompositeTransform(transform_list)\n",
    "\n",
    "def get_conditional_base_flow(\n",
    "    input_dim,\n",
    "    context_dim,\n",
    "    nstack,\n",
    "    nnodes,\n",
    "    nblocks,\n",
    "    tail_bound,\n",
    "    nbins,\n",
    "    activation,\n",
    "    dropout_probability,\n",
    "):\n",
    "    flow = Flow(\n",
    "        spline_inn(\n",
    "            input_dim,\n",
    "            nodes=nnodes,\n",
    "            num_blocks=nblocks,\n",
    "            num_stack=nstack,\n",
    "            tail_bound=tail_bound,\n",
    "            activation=getattr(F, activation),\n",
    "            dropout_probability=dropout_probability,\n",
    "            num_bins=nbins,\n",
    "            context_features=context_dim,\n",
    "        ),\n",
    "        #ConditionalDiagonalNormal(\n",
    "        #    shape=[input_dim], context_encoder=nn.Linear(context_dim, 2 * input_dim)\n",
    "        #),\n",
    "        DiagonalGaussian(shape=[input_dim], mean=0., std=0.25),\n",
    "    )\n",
    "\n",
    "    return flow\n",
    "\n",
    "def make_base_flow_and_train(loader, test_loader, df_test):\n",
    "    flow = get_conditional_base_flow(\n",
    "        input_dim=ninput,\n",
    "        context_dim=ncontext,\n",
    "        nstack=2,\n",
    "        nnodes=8,\n",
    "        nblocks=4,\n",
    "        tail_bound=1.0,\n",
    "        nbins=8,\n",
    "        activation=\"relu\",\n",
    "        dropout_probability=0.1,\n",
    "    )\n",
    "    flow = flow.to(device)\n",
    "    optimizer = optim.Adam(flow.parameters())\n",
    "\n",
    "    train_history, test_history = [], []\n",
    "    for epoch in range(epochs + 1):\n",
    "        print(epoch)\n",
    "        train_losses, test_losses = [], []\n",
    "\n",
    "        # train\n",
    "        for ab, xy, weights in loader:\n",
    "            loss = -flow.log_prob(inputs=xy, context=ab) * weights\n",
    "            loss = loss.mean()\n",
    "            train_losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        train_history.append(epoch_train_loss)\n",
    "\n",
    "        # test\n",
    "        print('testing')\n",
    "        for ab, xy, weights in test_loader:\n",
    "            with torch.no_grad():\n",
    "                loss = -flow.log_prob(inputs=xy, context=ab) * weights\n",
    "                loss = loss.mean()\n",
    "                test_losses.append(loss.item())\n",
    "        \n",
    "        epoch_test_loss = np.mean(test_losses)\n",
    "        test_history.append(epoch_test_loss)\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            print(\"plotting\")\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            a, b, x, y = df_test['a'].values, df_test['b'].values, df_test['x'].values, df_test['y'].values\n",
    "            xy_sample = flow.sample(1, context=torch.tensor(df_test[['a', 'b']].values, dtype=torch.float32).to(device)).reshape(-1, ninput)\n",
    "            x_sample = xy_sample[:, 0].detach().cpu().numpy()\n",
    "            y_sample = xy_sample[:, 1].detach().cpu().numpy()\n",
    "            x_min = min(x.min(), x_sample.min())\n",
    "            x_max = max(x.max(), x_sample.max())\n",
    "            ax1.hist(x, bins=100, range=(x_min, x_max), density=True, alpha=0.5, label='sample');\n",
    "            ax1.hist(x_sample, bins=100, range=(x_min, x_max), density=True, alpha=0.5, label='flow');\n",
    "            y_min = min(y.min(), y_sample.min())\n",
    "            y_max = max(y.max(), y_sample.max())\n",
    "            ax1.legend()\n",
    "            ax2.hist(y, bins=100, range=(y_min, y_max), density=True, alpha=0.5, label='sample');\n",
    "            ax2.hist(y_sample, bins=100, range=(y_min, y_max), density=True, alpha=0.5, label='flow');\n",
    "            ax2.legend()\n",
    "            # plot loss\n",
    "            ax3.plot(train_history, label='train')\n",
    "            ax3.plot(test_history, label='test')\n",
    "            ax3.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b74dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mc = make_base_flow_and_train(mc_loader_train, mc_loader_test, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMS_Thesis",
   "language": "python",
   "name": "cms_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
